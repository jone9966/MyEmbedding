{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"./config/config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1657756253908,
     "user": {
      "displayName": "정장일",
      "userId": "01548812619845711165"
     },
     "user_tz": -540
    },
    "id": "MGk_O1PwNByy"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  BertTokenizer\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, tokenizer, corpus, labels, maxlen=300):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.maxlen = maxlen\n",
    "        self.inputs = [self.tokenize(text) for text in corpus]\n",
    "        self.labels = [np.array(label) for label in labels]\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        data = self.tokenizer(data, max_length=self.maxlen, padding=\"max_length\", truncation=True,)\n",
    "        return np.array(data['input_ids']), np.array(data['token_type_ids']), np.array(data['attention_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1657756131756,
     "user": {
      "displayName": "정장일",
      "userId": "01548812619845711165"
     },
     "user_tz": -540
    },
    "id": "ndzqzepcM3Mq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size, num_classes, dr_rate=0.5):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=dr_rate)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        _, pooler = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        output = self.dropout(pooler)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 10907,
     "status": "ok",
     "timestamp": 1657756143358,
     "user": {
      "displayName": "정장일",
      "userId": "01548812619845711165"
     },
     "user_tz": -540
    },
    "id": "8v7hPluoZAzA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "bert = BertModel.from_pretrained(\"beomi/kcbert-base\", return_dict=False)\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 4\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "learning_rate = 5e-5\n",
    "\n",
    "df_token = pd.read_csv(config['path']['train'])\n",
    "corpus = [t for t in df_token['text']]\n",
    "label = to_categorical(LabelEncoder().fit_transform(df_token['label']))\n",
    "\n",
    "train_dataset = BERTDataset(tokenizer, corpus, label, maxlen=128)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1614,
     "status": "ok",
     "timestamp": 1657756223667,
     "user": {
      "displayName": "정장일",
      "userId": "01548812619845711165"
     },
     "user_tz": -540
    },
    "id": "hfuMvkYcZW_h",
    "outputId": "0f6aad12-604f-49e3-90d3-fcabc1a8ded2"
   },
   "outputs": [],
   "source": [
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "model = BERTClassifier(bert, hidden_size=768, num_classes=8)\n",
    "model.to(device)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 732904,
     "status": "ok",
     "timestamp": 1657757199984,
     "user": {
      "displayName": "정장일",
      "userId": "01548812619845711165"
     },
     "user_tz": -540
    },
    "id": "rZ0ppaTQNTm8",
    "outputId": "d3ebe973-fa59-4993-d8f9-fab00d39421d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 549/549 [01:09<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06414136195514827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 549/549 [01:10<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.075216880026873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 549/549 [01:07<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12975959540356968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 549/549 [01:10<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16630310353939887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 549/549 [01:04<00:00,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17875592456867345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_id, (inputs, label) in enumerate(tqdm(train_dataloader, ncols=0)):\n",
    "        input_ids = inputs[0].long().to(device)\n",
    "        token_type_ids = inputs[1].long().to(device)\n",
    "        attention_mask = inputs[2].long().to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(sum(losses)/len(losses))\n",
    "    state = {'Epoch': epoch,\n",
    "             'State_dict': model.state_dict(),\n",
    "             'Optimizer': optimizer.state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(config['path']['test'])\n",
    "test_corpus = [t for t in df_test['text']]\n",
    "test_label = to_categorical(LabelEncoder().fit_transform(df_test['label']))\n",
    "\n",
    "test_dataset = BERTDataset(tokenizer, test_corpus, test_label, maxlen=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1657757405190,
     "user": {
      "displayName": "정장일",
      "userId": "01548812619845711165"
     },
     "user_tz": -540
    },
    "id": "X4WEbWSmWMtB",
    "outputId": "4a52335a-af69-4d5c-a8cd-486d963a1eb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 942/942 [00:19<00:00, 48.03it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "fail = []\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, label) in enumerate(tqdm(test_dataloader, ncols=0)):\n",
    "        input_ids = inputs[0].long().to(device)\n",
    "        token_type_ids = inputs[1].long().to(device)\n",
    "        attention_mask = inputs[2].long().to(device)\n",
    "\n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        pred = np.argmax(out.cpu().detach().numpy(), axis=1).item()\n",
    "        label = np.argmax(label, axis=1).item()\n",
    "        if pred != label: fail.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1602972399150743\n"
     ]
    }
   ],
   "source": [
    "print(len(fail)/len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정부 가일 부터 코로나 로 피해 를 입은소 상공 인 과 고용 취약 계층 에 버팀목 ...</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>한국 쇼트트랙 간판 선수 심석희 를 성폭행 한 혐의 로 기소 된 조 재범 전 쇼트트...</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>여자 쇼트트랙 국가대표 심석희 를 상습 성폭행 한 혐의 로 기소 된 조 재범 전 국...</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>화성시 가 봄철 미세먼지 계절 관리제 기간 을 맞아이 달말까지 농촌 지역 폐기물 불...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>앙겔라 메르켈 독일 총리 가 지난 일 현 지 시간 백신 생산 업체 및 주 총리 들 ...</td>\n",
       "      <td>economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>웹툰 을 서비스 하 는 카카오페이지 와 음원 및 드라마 제작 까지 아우르는 카카오 ...</td>\n",
       "      <td>entertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>오늘 군 에서는 코로나 신규 확 진자 가명 발생 해 누적 확 진자 가명 으로 늘어났...</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>행정구역 합치는 특별 지자체 적 극지 원 사견 전 제 경기도 는 남북 으로 나눠야 ...</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>일 본 수도권 일부 지역 에 신종 코로나바이러스 감염증 코로나 긴급 사태 가 발령 ...</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>서 울 뉴시스 중국 외교부 가중 국판 백 신여 권 에 대해 추가 적 인 설명 을 내...</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          label\n",
       "4    정부 가일 부터 코로나 로 피해 를 입은소 상공 인 과 고용 취약 계층 에 버팀목 ...        society\n",
       "19   한국 쇼트트랙 간판 선수 심석희 를 성폭행 한 혐의 로 기소 된 조 재범 전 쇼트트...        society\n",
       "20   여자 쇼트트랙 국가대표 심석희 를 상습 성폭행 한 혐의 로 기소 된 조 재범 전 국...        society\n",
       "36   화성시 가 봄철 미세먼지 계절 관리제 기간 을 맞아이 달말까지 농촌 지역 폐기물 불...       politics\n",
       "49   앙겔라 메르켈 독일 총리 가 지난 일 현 지 시간 백신 생산 업체 및 주 총리 들 ...        economy\n",
       "..                                                 ...            ...\n",
       "923  웹툰 을 서비스 하 는 카카오페이지 와 음원 및 드라마 제작 까지 아우르는 카카오 ...      entertain\n",
       "933  오늘 군 에서는 코로나 신규 확 진자 가명 발생 해 누적 확 진자 가명 으로 늘어났...  international\n",
       "936  행정구역 합치는 특별 지자체 적 극지 원 사견 전 제 경기도 는 남북 으로 나눠야 ...        society\n",
       "937  일 본 수도권 일부 지역 에 신종 코로나바이러스 감염증 코로나 긴급 사태 가 발령 ...        society\n",
       "940  서 울 뉴시스 중국 외교부 가중 국판 백 신여 권 에 대해 추가 적 인 설명 을 내...  international\n",
       "\n",
       "[151 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN74cqIp13QaLUE209jfDuE",
   "mount_file_id": "1QTAaBo_UX6syFYfxm3fR-kAIWEmF1cR-",
   "name": "Bert.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
